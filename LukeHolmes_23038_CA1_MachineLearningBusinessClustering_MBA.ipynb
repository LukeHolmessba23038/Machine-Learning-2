{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7bf539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "df = pd.read_excel('data (2).xlsx')\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "print(df.info())\n",
    "\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a160ac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)\n",
    "\n",
    "df['Customer ID'] = df['Customer ID'].fillna(0).astype(int)\n",
    "\n",
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], errors='coerce')  # 'coerce' will set invalid parsing as NaT\n",
    "\n",
    "df['Country'] = df['Country'].astype('category')\n",
    "\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e053f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "df = df.dropna(subset=['Customer ID'])\n",
    "\n",
    "df = df[(df['Quantity'] > 0) & (df['Price'] > 0)]\n",
    "\n",
    "customer_df = df.groupby('Customer ID').agg(\n",
    "    TotalSpend=pd.NamedAgg(column='Price', aggfunc='sum'),\n",
    "    PurchaseFrequency=pd.NamedAgg(column='Invoice', aggfunc=pd.Series.nunique),\n",
    "    Recency=pd.NamedAgg(column='InvoiceDate', aggfunc=lambda x: (datetime.now() - x.max()).days)\n",
    ")\n",
    "\n",
    "customer_df['AverageBasketValue'] = customer_df['TotalSpend'] / customer_df['PurchaseFrequency']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc7d249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_product(description):\n",
    "    if pd.isna(description):\n",
    "        return 'Unknown'\n",
    "    description = description.lower()\n",
    "    if 'light' in description:\n",
    "        return 'Lighting'\n",
    "    elif 'frame' in description:\n",
    "        return 'Frames'\n",
    "    elif 'ceramic' in description:\n",
    "        return 'Ceramics'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "df['ProductCategory'] = df['Description'].apply(categorize_product)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b427698",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MonthOfPurchase'] = df['InvoiceDate'].dt.month\n",
    "df['DayOfWeek'] = df['InvoiceDate'].dt.dayofweek  \n",
    "df['TimeOfDay'] = df['InvoiceDate'].dt.hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd17f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['Country', 'ProductCategory'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a4b195",
   "metadata": {},
   "outputs": [],
   "source": [
    "variances = df.var()\n",
    "\n",
    "threshold = 0.01 * variances.max()\n",
    "\n",
    "low_variance_cols = variances[variances < threshold].index\n",
    "\n",
    "df = df.drop(columns=low_variance_cols)\n",
    "\n",
    "print(\"Columns removed due to low variance:\", low_variance_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b473321c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.describe())\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcdcbe8",
   "metadata": {},
   "source": [
    "Importing Libraries\n",
    "import pandas as pd: Imports the pandas library, a powerful tool for data manipulation and analysis, and gives it the alias pd.\n",
    "import seaborn as sns: Imports the seaborn library, used for statistical data visualization, with the alias sns.\n",
    "import matplotlib.pyplot as plt: Imports the pyplot module from the matplotlib library, which provides MATLAB-like plotting framework.\n",
    "from sklearn.metrics import silhouette_score: Imports the silhouette_score function from scikit-learn's metrics module, used to evaluate the quality of clusters in clustering algorithms.\n",
    "Data Loading and Initial Exploration\n",
    "df = pd.read_excel('data (2).xlsx'): Loads an Excel file into a pandas DataFrame df.\n",
    "print(df.head()): Displays the first five rows of the DataFrame for a quick overview.\n",
    "print(df.info()): Provides a concise summary of the DataFrame, including the number of non-null entries for each column.\n",
    "print(df.describe()): Generates descriptive statistics that summarize the central tendency, dispersion, and shape of the datasetâ€™s distribution, excluding NaN values.\n",
    "Data Type Conversions and Cleaning\n",
    "df['Customer ID'] = df['Customer ID'].fillna(0).astype(int): Fills missing values in the 'Customer ID' column with 0 and converts the column to integer data type.\n",
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], errors='coerce'): Converts the 'InvoiceDate' column to datetime format, setting invalid parsing as Not a Time (NaT).\n",
    "df['Country'] = df['Country'].astype('category'): Converts the 'Country' column to a categorical data type for memory efficiency.\n",
    "Data Filtering and Aggregation\n",
    "Removing Rows: Rows with missing 'Customer ID' and those with non-positive 'Quantity' or 'Price' values are removed to clean the dataset.\n",
    "Aggregating Data: The dataset is aggregated at the customer level to calculate total spend, purchase frequency, and recency of purchase. A new column 'AverageBasketValue' is also calculated.\n",
    "Feature Engineering\n",
    "Categorizing Products: A function categorize_product is defined and applied to the 'Description' column to categorize products into predefined categories.\n",
    "Extracting Date Features: New columns are created to capture the month of purchase, day of the week, and time of day from the 'InvoiceDate' column.\n",
    "One-Hot Encoding: The 'Country' and 'ProductCategory' columns are transformed into dummy/indicator variables for machine learning readiness.\n",
    "Variance Thresholding: Columns with low variance are identified and removed, as they might not be informative for predictive modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9303e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585366ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.describe())\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a801120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "\n",
    "corr_matrix = df.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool_))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]\n",
    "\n",
    "\n",
    "print(\"Features to drop due to high correlation:\", to_drop)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9de7c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "numeric_cols = customer_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "customer_df_scaled = scaler.fit_transform(customer_df[numeric_cols])\n",
    "\n",
    "customer_df_scaled = pd.DataFrame(customer_df_scaled, columns=numeric_cols, index=customer_df.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceec689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.95)  \n",
    "customer_df_reduced = pca.fit_transform(customer_df_scaled)\n",
    "\n",
    "print(f\"PCA reduced the feature space to {customer_df_reduced.shape[1]} dimensions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a5e2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "clusters = kmeans.fit_predict(customer_df_reduced)\n",
    "\n",
    "customer_df['Cluster'] = clusters\n",
    "\n",
    "print(kmeans.cluster_centers_)\n",
    "\n",
    "for i in range(5):  \n",
    "    print(f\"\\nCluster {i} characteristics:\")\n",
    "    cluster_members = customer_df[customer_df['Cluster'] == i]\n",
    "    print(cluster_members.describe())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b6f62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "linked = linkage(customer_df_reduced, 'ward')\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "clusters_hc = fcluster(linked, 5, criterion='maxclust')\n",
    "customer_df['Cluster_HC'] = clusters_hc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7525067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "clusters_dbscan = dbscan.fit_predict(customer_df_reduced)\n",
    "\n",
    "customer_df['Cluster_DBSCAN'] = clusters_dbscan\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054e6110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3954fd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "linked = linkage(customer_df_reduced, method='ward')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da362ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(linked,\n",
    "           orientation='top',\n",
    "           distance_sort='descending',\n",
    "           show_leaf_counts=True)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba50942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "n_components_range = range(1, 10)  \n",
    "bic_scores = []\n",
    "\n",
    "for n_components in n_components_range:\n",
    "    gmm = GaussianMixture(n_components=n_components, random_state=42)\n",
    "    gmm.fit(customer_df_reduced)\n",
    "    bic_scores.append(gmm.bic(customer_df_reduced))\n",
    "\n",
    "plt.plot(n_components_range, bic_scores, marker='o')\n",
    "plt.title('BIC Scores by Number of Components')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('BIC Score')\n",
    "plt.show()\n",
    "\n",
    "optimal_n_components = n_components_range[np.argmin(bic_scores)]\n",
    "print(f\"Optimal number of components: {optimal_n_components}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a7721e",
   "metadata": {},
   "source": [
    "Feature Selection\n",
    "High Correlation Feature Removal: Identifies and drops highly correlated features (correlation > 0.8) to avoid multicollinearity, which can distort the results of some models.\n",
    "Scaling\n",
    "Standard Scaling: Applies StandardScaler to normalize the numeric columns of customer_df, ensuring that each feature contributes equally to the analysis.\n",
    "Dimensionality Reduction\n",
    "PCA (Principal Component Analysis): Reduces the dimensionality of the scaled data while retaining 95% of the variance, making the dataset easier to work with and less prone to overfitting.\n",
    "Clustering with KMeans\n",
    "KMeans Clustering: Partitions the data into 5 clusters using the PCA-reduced features, then analyzes the characteristics of each cluster.\n",
    "Hierarchical Clustering\n",
    "Dendrogram: Visualizes the results of hierarchical clustering, providing insights into how the dataset could be grouped at various levels of granularity.\n",
    "DBSCAN Clustering\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise): Applies DBSCAN to identify core samples of high density and expand clusters from them, useful for data with clusters of similar density.\n",
    "Gaussian Mixture Models (GMM)\n",
    "BIC for GMM: Uses the Bayesian Information Criterion (BIC) to determine the optimal number of components for a Gaussian Mixture Model, balancing model complexity with goodness of fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2917ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "agglo = AgglomerativeClustering(n_clusters=5)  # Adjust n_clusters based on your requirement\n",
    "clusters_agglo = agglo.fit_predict(customer_df_reduced)\n",
    "\n",
    "customer_df['Cluster_Agglo'] = clusters_agglo\n",
    "\n",
    "silhouette_agglo = silhouette_score(customer_df_reduced, clusters_agglo)\n",
    "print(f\"Silhouette Score for Agglomerative Clustering: {silhouette_agglo}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a903e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "affinity = AffinityPropagation(random_state=42)\n",
    "clusters_affinity = affinity.fit_predict(customer_df_reduced)\n",
    "\n",
    "customer_df['Cluster_Affinity'] = clusters_affinity\n",
    "\n",
    "silhouette_affinity = silhouette_score(customer_df_reduced, clusters_affinity)\n",
    "print(f\"Silhouette Score for Affinity Propagation: {silhouette_affinity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719903fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift\n",
    "\n",
    "mean_shift = MeanShift()\n",
    "clusters_mean_shift = mean_shift.fit_predict(customer_df_reduced)\n",
    "\n",
    "customer_df['Cluster_MeanShift'] = clusters_mean_shift\n",
    "\n",
    "silhouette_mean_shift = silhouette_score(customer_df_reduced, clusters_mean_shift)\n",
    "print(f\"Silhouette Score for Mean Shift: {silhouette_mean_shift}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ccf71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "\n",
    "distance_threshold = 50  \n",
    "clusters = fcluster(linked, distance_threshold, criterion='distance')\n",
    "\n",
    "customer_df['Cluster_Labels'] = clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6ad2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(customer_df_reduced[:, 0], customer_df_reduced[:, 1], c=customer_df['Cluster'], cmap='viridis')\n",
    "plt.title('Clusters visualization in 2D')\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "silhouette_kmeans = silhouette_score(customer_df_reduced, customer_df['Cluster'])\n",
    "silhouette_hc = silhouette_score(customer_df_reduced, customer_df['Cluster_HC'])\n",
    "\n",
    "print(f\"Silhouette Score for K-Means: {silhouette_kmeans}\")\n",
    "print(f\"Silhouette Score for Hierarchical Clustering: {silhouette_hc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00ec536",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(customer_df_reduced[:, 0], customer_df_reduced[:, 1], c=customer_df['Cluster_DBSCAN'], cmap='viridis', s=50)\n",
    "plt.title('DBSCAN Clusters visualization in 2D')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef73bf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nearest_neighbors = NearestNeighbors(n_neighbors=5)\n",
    "neighbors = nearest_neighbors.fit(customer_df_reduced)\n",
    "distances, indices = neighbors.kneighbors(customer_df_reduced)\n",
    "distances = np.sort(distances[:, -1])\n",
    "\n",
    "plt.plot(distances)\n",
    "plt.title('k-Distance Graph')\n",
    "plt.xlabel('Points sorted by distance')\n",
    "plt.ylabel('5th Nearest Neighbor Distance')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f773a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "customer_df_reduced = pca.fit_transform(customer_df.drop('Cluster', axis=1))\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(customer_df_reduced[:, 0], customer_df_reduced[:, 1], c=customer_df['Cluster'], cmap='viridis', s=50, alpha=0.6)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('Customer Segments')\n",
    "plt.colorbar(label='Cluster Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddb251f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "dbscan = DBSCAN(eps=4200, min_samples=5)  \n",
    "clusters_dbscan = dbscan.fit_predict(customer_df_reduced)\n",
    "\n",
    "\n",
    "customer_df['Cluster_DBSCAN'] = clusters_dbscan\n",
    "\n",
    "\n",
    "n_clusters = len(set(clusters_dbscan)) - (1 if -1 in clusters_dbscan else 0)\n",
    "n_noise = list(clusters_dbscan).count(-1)\n",
    "\n",
    "print(f\"Estimated number of clusters: {n_clusters}\")\n",
    "print(f\"Estimated number of noise points: {n_noise}\")\n",
    "\n",
    "\n",
    "if n_clusters > 1:\n",
    "    silhouette_dbscan = silhouette_score(customer_df_reduced[clusters_dbscan != -1], clusters_dbscan[clusters_dbscan != -1])\n",
    "    print(f\"Silhouette Score for DBSCAN: {silhouette_dbscan}\")\n",
    "else:\n",
    "    print(\"Not enough clusters for Silhouette Score.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ad7e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette_kmeans = silhouette_score(customer_df_reduced, customer_df['Cluster'])\n",
    "print(f\"Silhouette Score for K-Means: {silhouette_kmeans}\")\n",
    "\n",
    "silhouette_hc = silhouette_score(customer_df_reduced, customer_df['Cluster_HC'])\n",
    "print(f\"Silhouette Score for Hierarchical Clustering: {silhouette_hc}\")\n",
    "\n",
    "if len(set(customer_df['Cluster_DBSCAN']) - {-1}) > 1:\n",
    "    silhouette_dbscan = silhouette_score(customer_df_reduced[customer_df['Cluster_DBSCAN'] != -1], customer_df['Cluster_DBSCAN'][customer_df['Cluster_DBSCAN'] != -1])\n",
    "    print(f\"Silhouette Score for DBSCAN: {silhouette_dbscan}\")\n",
    "else:\n",
    "    print(\"DBSCAN did not form distinct clusters, so Silhouette Score is not applicable.\")\n",
    "\n",
    "silhouette_agglo = silhouette_score(customer_df_reduced, customer_df['Cluster_Agglo'])\n",
    "print(f\"Silhouette Score for Agglomerative Clustering: {silhouette_agglo}\")\n",
    "\n",
    "silhouette_affinity = silhouette_score(customer_df_reduced, customer_df['Cluster_Affinity'])\n",
    "print(f\"Silhouette Score for Affinity Propagation: {silhouette_affinity}\")\n",
    "\n",
    "silhouette_mean_shift = silhouette_score(customer_df_reduced, customer_df['Cluster_MeanShift'])\n",
    "print(f\"Silhouette Score for Mean Shift: {silhouette_mean_shift}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32145da",
   "metadata": {},
   "source": [
    "Feature Selection\n",
    "High Correlation Feature Removal: Identifies and drops highly correlated features (correlation > 0.8) to avoid multicollinearity, which can distort the results of some models.\n",
    "Scaling\n",
    "Standard Scaling: Applies StandardScaler to normalize the numeric columns of customer_df, ensuring that each feature contributes equally to the analysis.\n",
    "Dimensionality Reduction\n",
    "PCA (Principal Component Analysis): Reduces the dimensionality of the scaled data while retaining 95% of the variance, making the dataset easier to work with and less prone to overfitting.\n",
    "Clustering with KMeans\n",
    "KMeans Clustering: Partitions the data into 5 clusters using the PCA-reduced features, then analyzes the characteristics of each cluster.\n",
    "Hierarchical Clustering\n",
    "Dendrogram: Visualizes the results of hierarchical clustering, providing insights into how the dataset could be grouped at various levels of granularity.\n",
    "DBSCAN Clustering\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise): Applies DBSCAN to identify core samples of high density and expand clusters from them, useful for data with clusters of similar density.\n",
    "Gaussian Mixture Models (GMM)\n",
    "BIC for GMM: Uses the Bayesian Information Criterion (BIC) to determine the optimal number of components for a Gaussian Mixture Model, balancing model complexity with goodness of fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d27171",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = kmeans.cluster_centers_\n",
    "print(\"Cluster centroids:\\n\", centroids)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(customer_df_reduced[:, 0], customer_df_reduced[:, 1], c=customer_df['Cluster'], cmap='viridis', s=50)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=200, alpha=0.5, marker='X')  # Mark centroids\n",
    "plt.title('K-Means Clusters Visualization in 2D PCA Space')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.show()\n",
    "\n",
    "for i in range(kmeans.n_clusters):\n",
    "    cluster_members = customer_df[customer_df['Cluster'] == i]\n",
    "    print(f\"\\nCluster {i} characteristics:\")\n",
    "    print(cluster_members[numeric_cols].describe())  # Summary statistics for numerical features in each cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba19796",
   "metadata": {},
   "source": [
    "Cluster 0\n",
    "Size: Large (2904 members)\n",
    "Total Spend: Moderate average spend, with a wide range, indicating a mix of spending behaviors.\n",
    "Purchase Frequency: Moderate, suggesting occasional to regular purchasers.\n",
    "Recency: A wider range in recency, with some customers not having purchased recently.\n",
    "Average Basket Value: Varied, with some high-value purchases.\n",
    "This cluster might represent a \"General\" segment with a broad mix of behaviors but leaning towards moderate spend and frequency.\n",
    "\n",
    "Cluster 1\n",
    "Size: Very small (1 member)\n",
    "Total Spend: Extremely high, suggesting this might be an outlier or a very high-value customer.\n",
    "Purchase Frequency: Extremely high, indicating frequent transactions.\n",
    "Recency: Purchased very recently.\n",
    "Average Basket Value: High.\n",
    "Given its size, this cluster could be an \"Outlier\" or \"High-Value\" segment, possibly representing bulk purchases or institutional buying.\n",
    "\n",
    "Cluster 2\n",
    "Size: Medium (815 members)\n",
    "Total Spend: Lower average spend, indicating smaller transactions.\n",
    "Purchase Frequency: Lower, suggesting less frequent purchases.\n",
    "Recency: More recent purchases.\n",
    "Average Basket Value: Moderate, with some higher-value baskets.\n",
    "This cluster might be \"Occasional Shoppers\" with recent, less frequent, and smaller transactions.\n",
    "\n",
    "Cluster 3\n",
    "Size: Very small (2 members)\n",
    "Total Spend: High, likely indicating significant transactions.\n",
    "Purchase Frequency: Very low, suggesting one-off or rare purchases.\n",
    "Recency: Least recent purchasers.\n",
    "Average Basket Value: Extremely high, suggesting premium or bulk purchases.\n",
    "Like Cluster 1, this is likely an \"Outlier\" or \"Premium\" segment, possibly representing large, infrequent transactions.\n",
    "\n",
    "Cluster 4\n",
    "Size: Medium (591 members)\n",
    "Total Spend: Low, indicating smaller transaction sizes.\n",
    "Purchase Frequency: Low, suggesting infrequent purchases.\n",
    "Recency: Least recent, indicating a lapse since the last purchase.\n",
    "Average Basket Value: Relatively low, consistent with smaller, infrequent transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c283be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "customer_df_tsne = tsne.fit_transform(customer_df_reduced)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(customer_df_tsne[:, 0], customer_df_tsne[:, 1], c=customer_df['Cluster'], cmap='viridis', s=50)\n",
    "plt.title('Clusters Visualization with t-SNE')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414cf7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(kmeans.n_clusters):\n",
    "    cluster_members = customer_df[customer_df['Cluster'] == i]\n",
    "    print(f\"\\nProfile for Cluster {i}:\")\n",
    "    display(cluster_members.describe()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932d0cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "db_index = davies_bouldin_score(customer_df_reduced, customer_df['Cluster'])\n",
    "print(f\"Davies-Bouldin Index: {db_index}\")\n",
    "\n",
    "ch_index = calinski_harabasz_score(customer_df_reduced, customer_df['Cluster'])\n",
    "print(f\"Calinski-Harabasz Index: {ch_index}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3fdf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for train_index, test_index in kf.split(customer_df_reduced):\n",
    "    X_train, X_test = customer_df_reduced[train_index], customer_df_reduced[test_index]\n",
    "    \n",
    "    kmeans_cv = KMeans(n_clusters=5, random_state=42)\n",
    "    kmeans_cv.fit(X_train)\n",
    "    \n",
    "    test_clusters = kmeans_cv.predict(X_test)\n",
    "    \n",
    "    silhouette_cv = silhouette_score(X_test, test_clusters)\n",
    "    print(f\"Silhouette Score on test split: {silhouette_cv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae19cb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "key_features = ['TotalSpend', 'PurchaseFrequency', 'Recency', 'AverageBasketValue']\n",
    "\n",
    "for i in range(customer_df['Cluster'].nunique()):\n",
    "    print(f\"\\n--- Profile for Cluster {i} ---\")\n",
    "    cluster_data = customer_df[customer_df['Cluster'] == i]\n",
    "\n",
    "    display(cluster_data[key_features].describe().round(2))\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(key_features), figsize=(20, 5), sharey=True)\n",
    "    fig.suptitle(f'Distribution of Key Features in Cluster {i}')\n",
    "\n",
    "    for ax, feature in zip(axes, key_features):\n",
    "        sns.histplot(cluster_data[feature], kde=True, ax=ax)\n",
    "        ax.set_title(feature)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ab51da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(customer_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e13d700",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = customer_df['Cluster'].nunique()\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    cluster_data = customer_df[customer_df['Cluster'] == i]\n",
    "    \n",
    "    average_spend = cluster_data['TotalSpend'].mean()\n",
    "    average_frequency = cluster_data['PurchaseFrequency'].mean()\n",
    "    average_recency = cluster_data['Recency'].mean()\n",
    "    average_basket_value = cluster_data['AverageBasketValue'].mean()\n",
    "\n",
    "    print(f\"\\n--- Cluster {i} Insights ---\")\n",
    "    print(f\"Average Spend: {average_spend:.2f}\")\n",
    "    print(f\"Average Purchase Frequency: {average_frequency:.2f}\")\n",
    "    print(f\"Average Recency: {average_recency:.2f} days ago\")\n",
    "    print(f\"Average Basket Value: {average_basket_value:.2f}\")\n",
    "\n",
    "   \n",
    "    if average_spend > customer_df['TotalSpend'].quantile(0.75):\n",
    "        print(\"Action: Consider premium offers or loyalty programs for high spenders.\")\n",
    "    elif average_frequency > customer_df['PurchaseFrequency'].quantile(0.75):\n",
    "        print(\"Action: Reward frequent shoppers with a loyalty program or exclusive deals.\")\n",
    "    elif average_recency > customer_df['Recency'].quantile(0.75):\n",
    "        print(\"Action: Re-engage customers who haven't shopped recently with a 'We Miss You' campaign.\")\n",
    "    else:\n",
    "        print(\"Action: Engage with standard promotions and aim to increase the basket value.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6223950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_0 = customer_df[customer_df['Cluster'] == 0]\n",
    "\n",
    "print(\"General Shopper Characteristics:\")\n",
    "print(cluster_0[['TotalSpend', 'PurchaseFrequency', 'Recency', 'AverageBasketValue']].describe())\n",
    "\n",
    "\n",
    "high_spend_customers = cluster_0[cluster_0['TotalSpend'] > cluster_0['TotalSpend'].quantile(0.75)]\n",
    "print(f\"\\nNumber of High Spend Customers in Cluster 0: {len(high_spend_customers)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd82da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_1 = customer_df[customer_df['Cluster'] == 1]\n",
    "\n",
    "print(\"Occasional Shopper Characteristics:\")\n",
    "print(cluster_1[['TotalSpend', 'PurchaseFrequency', 'Recency', 'AverageBasketValue']].describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3941324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_2 = customer_df[customer_df['Cluster'] == 2]\n",
    "\n",
    "print(\"High Roller Characteristics:\")\n",
    "print(cluster_2[['TotalSpend', 'PurchaseFrequency', 'Recency', 'AverageBasketValue']].describe())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbf0f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_3 = customer_df[customer_df['Cluster'] == 3]\n",
    "\n",
    "print(\"Bulk Buyer Characteristics:\")\n",
    "print(cluster_3[['TotalSpend', 'PurchaseFrequency', 'Recency', 'AverageBasketValue']].describe())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157e6277",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_4 = customer_df[customer_df['Cluster'] == 4]\n",
    "\n",
    "print(\"Infrequent Shopper Characteristics:\")\n",
    "print(cluster_4[['TotalSpend', 'PurchaseFrequency', 'Recency', 'AverageBasketValue']].describe())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5b6378",
   "metadata": {},
   "source": [
    "GPT\n",
    "This code block continues with the analysis of the customer dataset, focusing on visualizing the clusters formed by the K-Means algorithm, using dimensionality reduction techniques for visualization, and characterizing and evaluating the clusters through various metrics. Let's break it down:\n",
    "\n",
    "Visualizing Clusters\n",
    "Cluster Centroids: Prints the coordinates of the centroids of the clusters formed by K-Means.\n",
    "2D Visualization using PCA: Plots the clusters in a 2D space defined by the first two principal components, marking cluster centroids for visual reference.\n",
    "t-SNE for Cluster Visualization\n",
    "t-SNE (t-Distributed Stochastic Neighbor Embedding): A tool to visualize high-dimensional data in a lower-dimensional space. It's used here to further visualize the clusters in a 2D space, potentially revealing the structure at the global and local level.\n",
    "Cluster Characterization\n",
    "Key Features Summary: Prints descriptive statistics for numerical features within each cluster, helping to understand the defining characteristics of each cluster.\n",
    "Cluster Evaluation\n",
    "Davies-Bouldin Index: A metric evaluating intra-cluster similarity and inter-cluster differences. Lower values indicate better clustering.\n",
    "Calinski-Harabasz Index: Measures the cluster validity based on the ratio of between-cluster variance to within-cluster variance. Higher values typically indicate better-defined clusters.\n",
    "Cross-Validation with KMeans\n",
    "K-Fold Cross-Validation: Splits the dataset into k consecutive folds, then fits and predicts the K-Means model on these folds, evaluating the model's performance on unseen data using silhouette score.\n",
    "Distribution of Key Features in Clusters\n",
    "Histograms: Visualizes the distribution of key features like 'TotalSpend', 'PurchaseFrequency', 'Recency', and 'AverageBasketValue' across each cluster using histograms.\n",
    "Cluster Insights and Actions\n",
    "Cluster Insights: Calculates average values of key features for each cluster and suggests actions based on these insights, such as targeting high spenders with premium offers or re-engaging infrequent shoppers.\n",
    "Detailed Cluster Analysis\n",
    "General Characteristics: Provides descriptive statistics for each cluster, identifying patterns such as high spend, frequent purchases, recent activity, and average basket value.\n",
    "Specific Cluster Analysis\n",
    "Explores characteristics of specific clusters, identifying unique traits such as high spending, occasional shopping, bulk buying, and infrequent shopping behaviors, and suggests targeted strategies for each group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f6a09d",
   "metadata": {},
   "source": [
    "Market Basket Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdce1b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b35d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffdec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1b11c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_excel('data (2).xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8747daac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c63b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_str = df2.groupby('Invoice')['Description'].apply(lambda items: [str(item) for item in items]).tolist()\n",
    "\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions_str).transform(transactions_str)\n",
    "\n",
    "one_hot_df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95f7c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori\n",
    "frequent_itemsets = apriori(one_hot_df, min_support=0.01, use_colnames=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73980c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import association_rules\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
    "print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1312f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "min_support = 0.05\n",
    "\n",
    "frequent_itemsets = apriori(one_hot_df, min_support=min_support, use_colnames=True)\n",
    "\n",
    "print(\"Frequent Itemsets:\")\n",
    "print(frequent_itemsets)\n",
    "\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "min_confidence = 0.5\n",
    "min_lift = 1.2\n",
    "\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "\n",
    "rules = rules[rules['lift'] > min_lift]\n",
    "\n",
    "print(\"\\nAssociation Rules:\")\n",
    "print(rules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe4b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_support = 0.03  \n",
    "frequent_itemsets = apriori(one_hot_df, min_support=min_support, use_colnames=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99873fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_confidence = 0.2  \n",
    "min_lift = 1.0 \n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "rules = rules[rules['lift'] > min_lift]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffdf387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e80e46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets = apriori(one_hot_df, min_support=0.01, use_colnames=True)\n",
    "\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699b74d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None) \n",
    "print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af9a569",
   "metadata": {},
   "source": [
    "Initial Setup and Data Loading\n",
    "Importing Libraries: Necessary libraries like pandas and mlxtend (Machine Learning Extensions) are imported. mlxtend is particularly useful for its implementation of the Apriori algorithm and functions to generate association rules.\n",
    "Data Loading: Transactional data is loaded from an Excel file using pd.read_excel. This data likely includes transactions where each row represents an item in a transaction, identified by an 'Invoice' number and described by a 'Description'.\n",
    "Data Preprocessing for MBA\n",
    "Transaction List Creation: The transactions are grouped by 'Invoice', and the 'Description' of items in each transaction is aggregated into lists.\n",
    "Transaction Encoding: The TransactionEncoder from mlxtend.preprocessing is applied to these lists to create a one-hot encoded matrix, where each column represents an item, and each row represents a transaction, with 1 indicating the presence of the item in the transaction.\n",
    "Conversion to DataFrame: The encoded matrix is converted back to a pandas DataFrame, creating a binary matrix suitable for the Apriori algorithm.\n",
    "Applying the Apriori Algorithm\n",
    "Frequent Itemsets Generation: Using mlxtend.frequent_patterns.apriori, frequent itemsets are identified based on a specified min_support threshold. These itemsets are combinations of items that appear together in transactions with frequency above the threshold.\n",
    "Association Rules Generation: From the frequent itemsets, mlxtend.frequent_patterns.association_rules generates association rules that meet specified thresholds for metrics like confidence and lift. These rules indicate potential associations between items, with metrics providing insight into the strength and significance of these associations.\n",
    "Iterative Analysis\n",
    "The process is iteratively refined by adjusting parameters like min_support, min_confidence, and min_lift to explore different levels of item association and rule strength.\n",
    "Output and Interpretation\n",
    "Rules Display: The resulting association rules, along with their support, confidence, and lift metrics, are displayed. These metrics provide insights into the prevalence of rule antecedents and consequents in the dataset, the reliability of the rules, and the strength of the association compared to random chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43283162",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_value_rules = rules[(rules['confidence'] > 0.5) & (rules['lift'] > 1.2)]\n",
    "\n",
    "print(high_value_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f20397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule = high_value_rules.iloc[0]\n",
    "antecedents = rule['antecedents']\n",
    "consequents = rule['consequents']\n",
    "print(f\"Antecedents: {antecedents}\")\n",
    "print(f\"Consequents: {consequents}\")\n",
    "print(f\"Support: {rule['support']}\")\n",
    "print(f\"Confidence: {rule['confidence']}\")\n",
    "print(f\"Lift: {rule['lift']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5b5d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "display(high_value_rules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c16929",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, rule in rules.iterrows():\n",
    "    antecedents = ', '.join([str(i) for i in list(rule['antecedents'])])\n",
    "    consequents = ', '.join([str(i) for i in list(rule['consequents'])])\n",
    "    \n",
    "    print(f\"Rule #{index + 1}\")\n",
    "    print(f\"Antecedents: {antecedents}\")\n",
    "    print(f\"Consequents: {consequents}\")\n",
    "    print(f\"Support: {rule['support']:.4f}\")\n",
    "    print(f\"Confidence: {rule['confidence']:.4f}\")\n",
    "    print(f\"Lift: {rule['lift']:.4f}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efae664b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b846166",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install networkx matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be3f7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import pandas as pd\n",
    "\n",
    "simple_rules = rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].copy()\n",
    "\n",
    "simple_rules['antecedents'] = simple_rules['antecedents'].apply(lambda x: ', '.join(list(x)))\n",
    "simple_rules['consequents'] = simple_rules['consequents'].apply(lambda x: ', '.join(list(x)))\n",
    "\n",
    "simple_rules['rule'] = simple_rules['antecedents'] + \" -> \" + simple_rules['consequents']\n",
    "\n",
    "top_rules = simple_rules.nlargest(20, 'lift')\n",
    "\n",
    "chart = alt.Chart(top_rules).mark_circle(size=100).encode(\n",
    "    x='lift',\n",
    "    y='confidence',\n",
    "    size='support',\n",
    "    color='lift',\n",
    "    tooltip=['rule', 'lift', 'confidence', 'support']\n",
    ").properties(\n",
    "    width=700,\n",
    "    height=400,\n",
    "    title='Top 20 Association Rules by Lift'\n",
    ")\n",
    "\n",
    "chart.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ae0540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "top_lift_rules = simple_rules.nlargest(10, 'lift')[['rule', 'lift']]\n",
    "\n",
    "chart = alt.Chart(top_lift_rules).mark_bar().encode(\n",
    "    x=alt.X('lift', sort=None),\n",
    "    y=alt.Y('rule', sort='-x'),\n",
    "    color='lift',\n",
    "    tooltip=['rule', 'lift']\n",
    ").properties(\n",
    "    width=700,\n",
    "    height=300,\n",
    "    title='Top 10 Association Rules by Lift'\n",
    ")\n",
    "\n",
    "chart.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc3d8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b970bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99695cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "rules['antecedents'] = rules['antecedents'].apply(lambda x: ', '.join(list(x)))\n",
    "rules['consequents'] = rules['consequents'].apply(lambda x: ', '.join(list(x)))\n",
    "\n",
    "fig = px.scatter(rules, x='support', y='confidence', color='lift',\n",
    "                 hover_data=['antecedents', 'consequents'])\n",
    "fig.update_layout(title='Interactive Plot of Association Rules',\n",
    "                  xaxis_title='Support',\n",
    "                  yaxis_title='Confidence')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ebce58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "\n",
    "\n",
    "frequent_itemsets = fpgrowth(one_hot_df, min_support=0.01, use_colnames=True)\n",
    "\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
    "\n",
    "filtered_rules = rules[(rules['lift'] >= 3) & (rules['confidence'] >= 0.3)]\n",
    "\n",
    "print(filtered_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa756f76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "high_lift_rules = rules[rules['lift'] > 10]\n",
    "\n",
    "sorted_high_lift_rules = high_lift_rules.sort_values(by='lift', ascending=False)\n",
    "\n",
    "print(\"Top Association Rules by Lift Value:\")\n",
    "print(sorted_high_lift_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head())\n",
    "\n",
    "top_rule = sorted_high_lift_rules.iloc[0]\n",
    "antecedents = ', '.join(list(top_rule['antecedents']))\n",
    "consequents = ', '.join(list(top_rule['consequents']))\n",
    "print(f\"\\nInterpretation Example:\")\n",
    "print(f\"Rule: {antecedents} -> {consequents}\")\n",
    "print(f\"Lift: {top_rule['lift']:.2f} suggests a very strong association between these items.\")\n",
    "print(\"This could indicate a thematic connection appealing to customer preferences for cohesive, themed purchases.\")\n",
    "print(\"Considering this rule's support and confidence, it strikes a balance between being a common enough pattern (support) and a reliable predictor (confidence).\")\n",
    "pd.set_option('display.max_rows', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e55d46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "sorted_high_lift_rules['antecedents_str'] = sorted_high_lift_rules['antecedents'].apply(lambda x: ', '.join(list(x)))\n",
    "sorted_high_lift_rules['consequents_str'] = sorted_high_lift_rules['consequents'].apply(lambda x: ', '.join(list(x)))\n",
    "\n",
    "fig = go.Figure(data=[go.Table(\n",
    "    header=dict(values=['Antecedents', 'Consequents', 'Support', 'Confidence', 'Lift'],\n",
    "                fill_color='paleturquoise',\n",
    "                align='left'),\n",
    "    cells=dict(values=[sorted_high_lift_rules['antecedents_str'].head(),\n",
    "                       sorted_high_lift_rules['consequents_str'].head(),\n",
    "                       sorted_high_lift_rules['support'].head().apply(lambda x: f\"{x:.3f}\"),\n",
    "                       sorted_high_lift_rules['confidence'].head().apply(lambda x: f\"{x:.3f}\"),\n",
    "                       sorted_high_lift_rules['lift'].head().apply(lambda x: f\"{x:.2f}\")],\n",
    "               fill_color='lavender',\n",
    "               align='left'))\n",
    "])\n",
    "\n",
    "top_rule = sorted_high_lift_rules.iloc[0]\n",
    "\n",
    "fig.add_annotation(dict(\n",
    "    showarrow=False,\n",
    "    xref='paper', yref='paper',\n",
    "    x=0, y=-0.2,\n",
    "    text=(\"Interpretation Example:<br>\"\n",
    "          f\"Rule: {top_rule['antecedents_str']} -> {top_rule['consequents_str']}<br>\"\n",
    "          f\"Lift: {top_rule['lift']:.2f} suggests a very strong association between these items.<br>\"\n",
    "          \"This could indicate a thematic connection appealing to customer preferences for cohesive, themed purchases.<br>\"\n",
    "          \"Considering this rule's support and confidence, it strikes a balance between being a common enough pattern (support) \"\n",
    "          \"and a reliable predictor (confidence).\"),\n",
    "    align='left'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    margin=dict(t=30, b=100), \n",
    "    title_text=\"Top Association Rules by Lift Value\"\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03b29b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Table(\n",
    "    header=dict(values=['Antecedents', 'Consequents', 'Support', 'Confidence', 'Lift'],\n",
    "                fill_color='paleturquoise',\n",
    "                align='left'),\n",
    "    cells=dict(values=[\n",
    "                       sorted_high_lift_rules.antecedents.head().apply(lambda x: ', '.join(list(x))),\n",
    "                       sorted_high_lift_rules.consequents.head().apply(lambda x: ', '.join(list(x))),\n",
    "                       sorted_high_lift_rules.support.head().apply(lambda x: f\"{x:.3f}\"),\n",
    "                       sorted_high_lift_rules.confidence.head().apply(lambda x: f\"{x:.3f}\"),\n",
    "                       sorted_high_lift_rules.lift.head().apply(lambda x: f\"{x:.2f}\")\n",
    "               ],\n",
    "               fill_color='lavender',\n",
    "               align='left'))\n",
    "])\n",
    "\n",
    "fig.add_annotation(dict(\n",
    "    showarrow=False,\n",
    "    xref='paper', yref='paper',\n",
    "    x=0, y=-0.2,\n",
    "    text=(\"Interpretation Example:<br>\"\n",
    "          f\"Rule: {' , '.join(list(antecedents))} -> {' , '.join(list(consequents))}<br>\"\n",
    "          f\"Lift: {top_rule['lift']:.2f} suggests a very strong association between these items.<br>\"\n",
    "          \"This could indicate a thematic connection appealing to customer preferences for cohesive, themed purchases.<br>\"\n",
    "          \"Considering this rule's support and confidence, it strikes a balance between being a common enough pattern (support) \"\n",
    "          \"and a reliable predictor (confidence).\"),\n",
    "    align='left'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    margin=dict(t=30, b=100), \n",
    "    title_text=\"Top Association Rules by Lift Value\"\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7c00c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "rules['antecedents'] = rules['antecedents'].apply(lambda x: ', '.join(list(x)))\n",
    "rules['consequents'] = rules['consequents'].apply(lambda x: ', '.join(list(x)))\n",
    "rules['label'] = rules['antecedents'] + \" -> \" + rules['consequents']\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=rules['support'],\n",
    "    y=rules['confidence'],\n",
    "    z=rules['lift'],\n",
    "    text=rules['label'],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=7,\n",
    "        color=rules['lift'],              \n",
    "        colorscale='Viridis', \n",
    "        opacity=0.8\n",
    "    )\n",
    ")])\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    title='3D Scatter Plot of Association Rules',\n",
    "    scene = dict(\n",
    "        xaxis_title='Support',\n",
    "        yaxis_title='Confidence',\n",
    "        zaxis_title='Lift'\n",
    "    ),\n",
    "    margin=dict(l=0, r=0, b=0, t=0) \n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07da98ef",
   "metadata": {},
   "source": [
    "High-Value Rules Filtering\n",
    "Filters association rules to retain those with high confidence and lift, indicating strong and potentially useful relationships between itemsets.\n",
    "Rule Analysis\n",
    "Selects the first high-value rule for a detailed analysis, breaking down its antecedents (if part), consequents (then part), support, confidence, and lift, providing a deeper understanding of individual rules.\n",
    "Visualization with Altair\n",
    "Uses Altair to visualize the top association rules by lift, highlighting the most significant relationships discovered in the dataset.\n",
    "Visualization with Plotly\n",
    "Employs Plotly to create interactive visualizations, such as a scatter plot and a 3D scatter plot, enabling a dynamic exploration of the rules based on support, confidence, and lift.\n",
    "Generates a table visualization of the top association rules for a clear and concise presentation of the findings.\n",
    "FP-Growth Algorithm\n",
    "Applies the FP-Growth algorithm, an efficient alternative to the Apriori algorithm for finding frequent itemsets, to the one-hot encoded dataset, followed by the generation of association rules from these itemsets.\n",
    "Detailed Rule Interpretation\n",
    "Provides a detailed interpretation of a top rule by analyzing its lift value and the implications of the antecedents leading to the consequents, aiding in the understanding of customer purchase behavior and thematic connections between items.\n",
    "Visualization Enhancements\n",
    "Enhances visualizations with annotations and table formats for better interpretation of the top association rules, making it easier to communicate findings to a non-technical audience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9c0171",
   "metadata": {},
   "source": [
    "References\n",
    "\n",
    "Altair Development Team (2024) Altair: Interactive Statistical Visualizations for Python. Available at: https://altair-viz.github.io (Accessed: 02 April 2024).\n",
    "\n",
    "Han, J., Pei, J., and Kamber, M. (2011) Data Mining: Concepts and Techniques. 3rd edn. Morgan Kaufmann Publishers.\n",
    "\n",
    "Harris, C.R., Millman, K.J., van der Walt, S.J. et al. (2020) 'Array programming with NumPy', Nature, 585, pp. 357â€“362. Available at: https://doi.org/10.1038/s41586-020-2649-2 (Accessed: 02 April 2024).\n",
    "\n",
    "Hunter, J.D. (2007) 'Matplotlib: A 2D Graphics Environment', Computing in Science & Engineering, 9(3), pp. 90-95. Available at: https://doi.org/10.1109/MCSE.2007.55 (Accessed: 02 April 2024).\n",
    "\n",
    "McKinney, W. (2010) Data Structures for Statistical Computing in Python. Proceedings of the 9th Python in Science Conference, pp. 51-56. Available at: https://conference.scipy.org/proceedings/scipy2010/mckinney.html (Accessed: 02 April 2024).\n",
    "\n",
    "Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E. (2011) 'Scikit-learn: Machine Learning in Python', Journal of Machine Learning Research, 12, pp. 2825-2830. Available at: http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html (Accessed: 02 April 2024).\n",
    "\n",
    "Raschka, S. (2015) Python Machine Learning. Packt Publishing Ltd.\n",
    "\n",
    "Seaborn Development Team (2024) Seaborn: Statistical Data Visualization. Available at: https://seaborn.pydata.org/ (Accessed: 02 April 2024).\n",
    "\n",
    "Waskom, M. et al. (2024) mwaskom/seaborn: v0.11.0 (September 2020), Zenodo. Available at: https://doi.org/10.5281/zenodo.4019143 (Accessed: 02 April 2024).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
